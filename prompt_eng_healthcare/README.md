# Using LLM prompts to analyze a healthcare dataset and compare how different prompt structures affect accuracy, hallucination, and clarity

Three prompt strategies—basic, role-based, and structured—were applied to the same healthcare dataset and evaluated using predefined criteria including accuracy, hallucination control, clarity, clinical relevance, and actionability. 

The structured prompt consistently achieved the highest average score across metrics, producing clearer and more reliable insights with fewer unsupported assumptions. 

Role-based prompting improved clinical context compared to the basic prompt but lacked consistent organization.

These results demonstrate that structured prompting significantly enhances the quality and trustworthiness of LLM-generated healthcare analytics.
